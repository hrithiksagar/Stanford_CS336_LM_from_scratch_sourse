{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr(0)\n",
    "# print(chr(0))\n",
    "# # \"this is a test\" + chr(0) + \"string\"\n",
    "# print(\"this is a test\" + chr(0) + \"string\")\n",
    "\n",
    "# # Output: this is a teststring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello! How are you? \\xe0\\xb0\\x8f\\xe0\\xb0\\x82 \\xe0\\xb0\\x9a\\xe0\\xb1\\x87\\xe0\\xb0\\xb8\\xe0\\xb1\\x8d\\xe0\\xb0\\xa4\\xe0\\xb1\\x81\\xe0\\xb0\\xa8\\xe0\\xb1\\x8d\\xe0\\xb0\\xa8\\xe0\\xb0\\xbe\\xe0\\xb0\\xb5\\xe0\\xb1\\x8d?'\n",
      "<class 'bytes'>\n",
      "36\n",
      "64\n",
      "Hello! How are you? ఏం చేస్తున్నావ్?\n"
     ]
    }
   ],
   "source": [
    "# UTF 8 Encode-Decode\n",
    "test_string = \"Hello! How are you? ఏం చేస్తున్నావ్?\"\n",
    "utf8_encode = test_string.encode(\"utf-8\")\n",
    "print(utf8_encode)\n",
    "print(type(utf8_encode))\n",
    "list(utf8_encode) # Get the byte values for the encoded string (integers from 0 to 255).\n",
    "# One byte does not necessarily correspond to one Unicode character!\n",
    "print(len(test_string))\n",
    "print(len(utf8_encode))\n",
    "print(utf8_encode.decode((\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: \n",
    "\n",
    "Deliverable: Write a function that, given path to an input text file, trains a (byte-level) BPE\n",
    "tokenizer. \n",
    "\n",
    "Your BPE training function should handle (at least) the following input parameters:\n",
    "\n",
    "*input_path*: str Path to a text file with BPE tokenizer training data.\n",
    "\n",
    "*vocab_size*: int A non-negative integer that defines the maximum final vocabulary size (including the initial byte vocabulary, vocabulary items produced from merging, and any special tokens).\n",
    "\n",
    "*special_tokens*: list[str] A list of strings to add to the vocabulary. These special tokens do not otherwise affect BPE training.\n",
    "\n",
    "Your BPE training function should return the resulting vocabulary and merges:\n",
    "\n",
    "*vocab*: dict[int, bytes] The tokenizer vocabulary, a mapping from int (token ID in the vocabu-\n",
    "lary) to bytes (token bytes).\n",
    "*merges*: list[tuple[bytes, bytes]] A list of BPE merges produced from training. Each list\n",
    "item is a tuple of bytes (<token1>, <token2>), representing that <token1> was merged with\n",
    "<token2>. The merges should be ordered by order of creation.\n",
    "\n",
    "To test your BPE training function against our provided tests, you will first need to implement the test adapter at [adapters.run_train_bpe]. Then, run pytest tests/test_train_bpe.py. Your implementation should be able to pass all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"this is a test\n",
    "this is another test\n",
    "byte pair encoding test\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't concat int to bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m     81\u001b[0m special_tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 83\u001b[0m vocab, merges \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary:\u001b[39m\u001b[38;5;124m\"\u001b[39m, vocab)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerges:\u001b[39m\u001b[38;5;124m\"\u001b[39m, merges)\n",
      "Cell \u001b[0;32mIn[24], line 66\u001b[0m, in \u001b[0;36mtrain_bpe\u001b[0;34m(input_path, vocab_size, special_tokens)\u001b[0m\n\u001b[1;32m     63\u001b[0m     best_pair \u001b[38;5;241m=\u001b[39m pairs\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Merge the pair\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     merges\u001b[38;5;241m.\u001b[39mappend(best_pair)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Step 5: Prepare the final vocab dictionary with int->byte mapping\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 19\u001b[0m, in \u001b[0;36mmerge_vocab\u001b[0;34m(pair, vocab)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Merge the most frequent byte pair into the vocabulary.\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m bigram \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, pair))\n\u001b[0;32m---> 19\u001b[0m replacement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m([\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m])\n\u001b[1;32m     21\u001b[0m new_vocab \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, freq \u001b[38;5;129;01min\u001b[39;00m vocab\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mTypeError\u001b[0m: can't concat int to bytes"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "def get_stats(text: List[List[bytes]]) -> Counter:\n",
    "    \"\"\"Get the frequencies of byte pairs in the text.\"\"\"\n",
    "    pairs = Counter()\n",
    "    for word in text:\n",
    "        for i in range(len(word) - 1):\n",
    "            pair = (word[i], word[i + 1])\n",
    "            pairs[pair] += 1\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(pair: Tuple[bytes, bytes], vocab: Dict[Tuple[bytes], int]) -> Dict[Tuple[bytes], int]:\n",
    "    \"\"\"Merge the most frequent byte pair into the vocabulary.\"\"\"\n",
    "    bigram = ' '.join(map(str, pair))\n",
    "    replacement = bytes([pair[0] + pair[1]])\n",
    "    \n",
    "    new_vocab = {}\n",
    "    for word, freq in vocab.items():\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word) - 1:\n",
    "            if word[i:i+2] == pair:\n",
    "                new_word.append(replacement)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        if i < len(word):\n",
    "            new_word.append(word[i])\n",
    "        new_vocab[tuple(new_word)] = freq\n",
    "    return new_vocab\n",
    "\n",
    "\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: List[str]) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"Train a byte-level BPE tokenizer.\"\"\"\n",
    "    # Step 1: Read the input text file\n",
    "    with open(input_path, 'rb') as f:\n",
    "        text = [list(line.strip()) for line in f]\n",
    "\n",
    "    # Step 2: Initialize the vocabulary with individual characters (bytes)\n",
    "    vocab = Counter(tuple(line) for line in text)\n",
    "    \n",
    "    # Step 3: Add special tokens to the vocabulary\n",
    "    special_tokens = [t.encode() for t in special_tokens]\n",
    "    \n",
    "    # Add special tokens with a count of 0 to the vocabulary\n",
    "    for token in special_tokens:\n",
    "        vocab[tuple([token])] = 0\n",
    "    \n",
    "    # Step 4: Perform BPE merging\n",
    "    merges = []\n",
    "    while len(vocab) < vocab_size:\n",
    "        pairs = get_stats(list(vocab.keys()))\n",
    "        \n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        # Get the most frequent pair\n",
    "        best_pair = pairs.most_common(1)[0][0]\n",
    "        \n",
    "        # Merge the pair\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "        merges.append(best_pair)\n",
    "    \n",
    "    # Step 5: Prepare the final vocab dictionary with int->byte mapping\n",
    "    final_vocab = {i: token for i, token in enumerate(vocab, start=len(special_tokens))}\n",
    "    \n",
    "    return final_vocab, merges\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example of running the function\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"input_text.txt\"\n",
    "    vocab_size = 10000\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]\n",
    "\n",
    "    vocab, merges = train_bpe(input_file, vocab_size, special_tokens)\n",
    "    \n",
    "    print(\"Vocabulary:\", vocab)\n",
    "    print(\"Merges:\", merges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
